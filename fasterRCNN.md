# **Faster RCNN : Towards Real-Time Object Detection with Region Proposal Networks**

## **Introduction**

#### _**WHY**_

Region proposals are the test-time computational bottleneck in state-of-the-art detection systems. The region proposal step still consumes as much running time as the detection network.

#### _**HOW**_

We introduce a Region Proposal Network (RPN) that shares full-image
convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional
network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection.

## **Model** 

Faster RCNN is built upon two  modules:

1. The first module is a deep fully convolutional network(Region Proposal Network or 'RPN') that proposes regions.
2. The second module is the Fast R-CNN detector that uses the proposed regions to detect objects.

Both modules together constitutes single, unified network for Object Detection as shown in the figure below:

![Faster RCNN](https://cdn-images-1.medium.com/max/1600/1*e6dx5qzUKWwasIVGSuCyDA.png)

<br>

#### **RPN(Regional Proposal Network)**
A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score. This is done using a CNN whose computations are shared with Fast RCNN object detection Network by sharing convolutional layers.

To generate region proposals, we slide an n x n window over feature map output of last Shared convolutional layer. This feature is fed into two sibling fully connected layers -- a box-regression layer (reg) and a box-classification layer (cls). 

Simplified view of RPN :

![RPN](https://jamiekang.github.io/media/2017-05-27-faster-r-cnn-rpn.jpg)



##### **Anchors in RPN**

At each sliding-window location, we simultaneously predict multiple region proposals, where the number of maximum possible proposals for each location is denoted as k. So the reg layer has 4k outputs encoding
the coordinates of k boxes, and the cls layer outputs 2k scores that estimate probability of object or not object for each proposal. These k proposals are parameterized relative to k reference boxes, known as *anchors*.

Anchors used in Faster RCNN in the paper is centred at sliding window and has 3 scales and 3 aspect ratios. Hence, 9 anchors per sliding location.

Note : Anchors are *translation invariant*. If one translates an object in an image, the proposal should translate and the same function should be able to predict the proposal in either location

Picturization of Anchors:

![Anchors](https://www.researchgate.net/publication/324060834/figure/fig1/AS:609163528916992@1522247311638/Anchor-boxes-of-the-Faster-R-CNN-R-CNN-region-proposal-convolution-neural-networks.png)

##### **Loss Function for RPN**

For training RPNs, we assign a binary class label (of being an object or not) to each anchor. We assign a positive label to two kinds of anchors: (i) the
anchor/anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box, or (ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box.

We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Other anchors doesn't contribute to the training of RPN.

With these definitions, we minimize the following objective function:

![Loss of RPN](https://image.slidesharecdn.com/pr12fasterrcnn170528-170802143120/95/faster-rcnn-pr012-28-638.jpg?cb=1504447138)

##### **Training for RPN**

The RPN can be trained end-to-end by back-propagation and stochastic gradient descent (SGD). Each mini-batch arises from a single image that contains many positive and negative example anchors. To prevent bias towards negative examples, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negative ones.

New layers are initialized using Gaussian distribution and the shared convolutional layers are initialized by pre-training a model on ImageNet classification.

#### **4-Step Alternate Training**

In this paper, a 4-step training algorithm is used to learn shared features through alternating optimization. These steps are :

1. Train the RPN as discussed in the last section.
2. Train the separate detection network by Fast RCNN using proposals generated by step 1 trained RPN. Note that *at this point, two networks doesn't share any convolutional layers*
3. Use detector network to initialize RPN training, keeping the shared convolutional layers fixed, only fine tuning RPN exclusive layers. *Now both networks share convolutional layers.*
4. Finally, keeping the shared convolutional layers fixed, we fine-tune the unique layers of Fast R-CNN.

#### **Results**

##### **On Pascal VOC**

On PASCAL VOC 2007 testset(trained on VOC 2007 trainval), having detectors as Fast R-CNN with ZF, but different region proposal methods, RPN method gives 1.3 mAP better than Selective search and edgebox. 

mAP of 78.8% on VOC 2007 test set, trained on COCO+07+12(union set of VOC 2007 trainval and VOC 2012 trainval) when detector is Fast RCNN and VGG16.

##### **On MS COCO**

Compared to Fast RCNN, Faster RCNN(on VGG-16) improves mAP@0.5 by 2.8% and mAP@[0.5, 0.95] by 2.2% on COCO test-dev when trained on COCO train dataset.

Only by replacing VGG-16 with a 101- layer residual net (ResNet-101), the Faster R-CNN system increases the mAP@0.5/mAP@[0.5, 0.95] from 41.5%/21.2% (VGG- 16) to 48.4%/27.2% (ResNet-101) on the COCO val set when trained on COCO train dataset.

##### **Running time of entire object detection system**

Using ZF Net, speed is 17 FPS and using VGG 16, it is 5 FPS.